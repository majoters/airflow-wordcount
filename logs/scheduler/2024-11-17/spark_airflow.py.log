[2024-11-17T11:25:28.316+0000] {processor.py:186} INFO - Started process (PID=179) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:25:28.317+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:25:28.318+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:28.318+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:25:28.327+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:25:28.380+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:28.380+0000] {override.py:1911} INFO - Created Permission View: can read on DAG:sparkingFlow
[2024-11-17T11:25:28.383+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:28.383+0000] {override.py:1911} INFO - Created Permission View: can edit on DAG:sparkingFlow
[2024-11-17T11:25:28.385+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:28.385+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG:sparkingFlow
[2024-11-17T11:25:28.386+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:28.386+0000] {override.py:1911} INFO - Created Permission View: can read on DAG Run:sparkingFlow
[2024-11-17T11:25:28.388+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:28.388+0000] {override.py:1911} INFO - Created Permission View: can delete on DAG Run:sparkingFlow
[2024-11-17T11:25:28.389+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:28.389+0000] {override.py:1911} INFO - Created Permission View: menu access on DAG Run:sparkingFlow
[2024-11-17T11:25:28.391+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:28.391+0000] {override.py:1911} INFO - Created Permission View: can create on DAG Run:sparkingFlow
[2024-11-17T11:25:28.391+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:28.391+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:25:28.396+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:28.396+0000] {dag.py:3262} INFO - Creating ORM DAG for sparkingFlow
[2024-11-17T11:25:28.400+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:28.400+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:25:28.405+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.092 seconds
[2024-11-17T11:25:58.607+0000] {processor.py:186} INFO - Started process (PID=184) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:25:58.607+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:25:58.610+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:58.610+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:25:58.616+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:25:58.628+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:58.628+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:25:58.636+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:25:58.636+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:25:58.643+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.039 seconds
[2024-11-17T11:26:28.888+0000] {processor.py:186} INFO - Started process (PID=195) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:26:28.889+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:26:28.892+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:26:28.892+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:26:28.903+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:26:28.920+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:26:28.920+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:26:28.930+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:26:28.930+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:26:28.937+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.055 seconds
[2024-11-17T11:26:59.241+0000] {processor.py:186} INFO - Started process (PID=200) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:26:59.242+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:26:59.245+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:26:59.245+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:26:59.257+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:26:59.275+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:26:59.275+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:26:59.286+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:26:59.286+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:26:59.293+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.059 seconds
[2024-11-17T11:27:29.592+0000] {processor.py:186} INFO - Started process (PID=212) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:27:29.593+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:27:29.596+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:27:29.596+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:27:29.606+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:27:29.623+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:27:29.623+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:27:29.635+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:27:29.635+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:27:29.642+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.056 seconds
[2024-11-17T11:27:39.135+0000] {processor.py:186} INFO - Started process (PID=179) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:27:39.136+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:27:39.137+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:27:39.137+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:27:39.143+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:27:39.154+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:27:39.154+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:27:39.165+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:27:39.165+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:27:39.171+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.039 seconds
[2024-11-17T11:28:09.428+0000] {processor.py:186} INFO - Started process (PID=197) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:28:09.429+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:28:09.431+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:28:09.431+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:28:09.441+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:28:09.455+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:28:09.455+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:28:09.465+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:28:09.465+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:28:09.472+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.047 seconds
[2024-11-17T11:28:39.708+0000] {processor.py:186} INFO - Started process (PID=208) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:28:39.708+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:28:39.710+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:28:39.710+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:28:39.720+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:28:39.735+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:28:39.735+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:28:39.746+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:28:39.746+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:28:39.753+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T11:29:10.002+0000] {processor.py:186} INFO - Started process (PID=219) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:29:10.003+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:29:10.006+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:29:10.006+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:29:10.018+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:29:10.036+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:29:10.036+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:29:10.047+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:29:10.047+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:29:10.055+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.058 seconds
[2024-11-17T11:29:40.310+0000] {processor.py:186} INFO - Started process (PID=224) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:29:40.310+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:29:40.312+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:29:40.312+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:29:40.318+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:29:40.329+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:29:40.329+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:29:40.339+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:29:40.339+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:29:40.349+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.042 seconds
[2024-11-17T11:30:10.620+0000] {processor.py:186} INFO - Started process (PID=229) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:30:10.620+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:30:10.623+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:30:10.623+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:30:10.634+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:30:10.651+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:30:10.651+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:30:10.662+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:30:10.662+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:30:10.669+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.054 seconds
[2024-11-17T11:30:40.922+0000] {processor.py:186} INFO - Started process (PID=234) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:30:40.922+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:30:40.925+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:30:40.924+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:30:40.934+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:30:40.949+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:30:40.949+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:30:40.958+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:30:40.958+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:30:40.965+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.048 seconds
[2024-11-17T11:31:11.262+0000] {processor.py:186} INFO - Started process (PID=239) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:31:11.263+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:31:11.265+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:31:11.265+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:31:11.275+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:31:11.291+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:31:11.291+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:31:11.301+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:31:11.301+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:31:11.308+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T11:31:41.588+0000] {processor.py:186} INFO - Started process (PID=244) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:31:41.588+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:31:41.590+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:31:41.590+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:31:41.599+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:31:41.614+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:31:41.614+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:31:41.625+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:31:41.625+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:31:41.632+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.049 seconds
[2024-11-17T11:32:11.907+0000] {processor.py:186} INFO - Started process (PID=249) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:32:11.908+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:32:11.910+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:32:11.910+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:32:11.920+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:32:11.935+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:32:11.935+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:32:11.946+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:32:11.946+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:32:11.953+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.050 seconds
[2024-11-17T11:32:42.358+0000] {processor.py:186} INFO - Started process (PID=254) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:32:42.359+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:32:42.361+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:32:42.361+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:32:42.372+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:32:42.397+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:32:42.397+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:32:42.407+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:32:42.407+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:32:42.413+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.060 seconds
[2024-11-17T11:33:12.703+0000] {processor.py:186} INFO - Started process (PID=259) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:33:12.704+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:33:12.706+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:33:12.706+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:33:12.716+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:33:12.736+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:33:12.736+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:33:12.746+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:33:12.746+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:33:12.752+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.055 seconds
[2024-11-17T11:33:43.020+0000] {processor.py:186} INFO - Started process (PID=264) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:33:43.021+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:33:43.022+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:33:43.022+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:33:43.029+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:33:43.046+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:33:43.046+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:33:43.055+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:33:43.055+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:33:43.062+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.045 seconds
[2024-11-17T11:34:13.324+0000] {processor.py:186} INFO - Started process (PID=269) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:34:13.325+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:34:13.326+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:34:13.326+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:34:13.334+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:34:13.350+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:34:13.350+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:34:13.360+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:34:13.360+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:34:13.367+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.046 seconds
[2024-11-17T11:34:43.607+0000] {processor.py:186} INFO - Started process (PID=274) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:34:43.607+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:34:43.610+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:34:43.610+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:34:43.619+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:34:43.636+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:34:43.636+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:34:43.645+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:34:43.645+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:34:43.652+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.050 seconds
[2024-11-17T11:35:13.913+0000] {processor.py:186} INFO - Started process (PID=279) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:35:13.913+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:35:13.917+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:35:13.917+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:35:13.930+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:35:13.956+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:35:13.956+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:35:13.971+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:35:13.971+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:35:13.978+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.072 seconds
[2024-11-17T11:35:44.298+0000] {processor.py:186} INFO - Started process (PID=284) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:35:44.298+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:35:44.302+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:35:44.302+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:35:44.313+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:35:44.333+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:35:44.332+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:35:44.342+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:35:44.342+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:35:44.348+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.059 seconds
[2024-11-17T11:36:14.696+0000] {processor.py:186} INFO - Started process (PID=289) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:36:14.697+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:36:14.699+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:36:14.699+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:36:14.708+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:36:14.728+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:36:14.728+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:36:14.738+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:36:14.738+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:36:14.744+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.053 seconds
[2024-11-17T11:36:45.078+0000] {processor.py:186} INFO - Started process (PID=294) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:36:45.078+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:36:45.081+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:36:45.081+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:36:45.090+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:36:45.110+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:36:45.110+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:36:45.121+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:36:45.121+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:36:45.127+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.057 seconds
[2024-11-17T11:37:15.461+0000] {processor.py:186} INFO - Started process (PID=299) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:37:15.462+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:37:15.464+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:37:15.464+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:37:15.474+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:37:15.494+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:37:15.494+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:37:15.504+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:37:15.504+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:37:15.510+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.055 seconds
[2024-11-17T11:37:45.847+0000] {processor.py:186} INFO - Started process (PID=304) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:37:45.848+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:37:45.849+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:37:45.849+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:37:45.860+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:37:45.873+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:37:45.873+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:37:45.882+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:37:45.882+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:37:45.888+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.045 seconds
[2024-11-17T11:38:16.270+0000] {processor.py:186} INFO - Started process (PID=309) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:38:16.271+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:38:16.274+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:38:16.273+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:38:16.289+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:38:16.305+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:38:16.305+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:38:16.315+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:38:16.315+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:38:16.321+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.057 seconds
[2024-11-17T11:38:46.683+0000] {processor.py:186} INFO - Started process (PID=314) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:38:46.683+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:38:46.685+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:38:46.685+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:38:46.701+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:38:46.716+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:38:46.716+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:38:46.726+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:38:46.726+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:38:46.733+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.057 seconds
[2024-11-17T11:39:17.025+0000] {processor.py:186} INFO - Started process (PID=319) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:39:17.025+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:39:17.027+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:39:17.027+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:39:17.039+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:39:17.053+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:39:17.053+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:39:17.064+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:39:17.063+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:39:17.070+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T11:39:47.388+0000] {processor.py:186} INFO - Started process (PID=324) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:39:47.389+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:39:47.391+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:39:47.391+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:39:47.407+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:39:47.422+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:39:47.422+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:39:47.432+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:39:47.432+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:39:47.439+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.056 seconds
[2024-11-17T11:40:17.786+0000] {processor.py:186} INFO - Started process (PID=329) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:40:17.786+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:40:17.788+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:40:17.788+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:40:17.802+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:40:17.816+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:40:17.815+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:40:17.825+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:40:17.825+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:40:17.831+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T11:40:48.193+0000] {processor.py:186} INFO - Started process (PID=334) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:40:48.193+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:40:48.197+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:40:48.196+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:40:48.253+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:40:48.287+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:40:48.287+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:40:48.305+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:40:48.305+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:40:48.313+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.129 seconds
[2024-11-17T11:41:18.543+0000] {processor.py:186} INFO - Started process (PID=339) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:41:18.544+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:41:18.547+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:41:18.547+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:41:18.557+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:41:18.573+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:41:18.573+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:41:18.585+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:41:18.585+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:41:18.592+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.056 seconds
[2024-11-17T11:41:48.896+0000] {processor.py:186} INFO - Started process (PID=344) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:41:48.896+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:41:48.898+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:41:48.898+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:41:48.908+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:41:48.926+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:41:48.926+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:41:48.939+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:41:48.939+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:41:48.946+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.056 seconds
[2024-11-17T11:42:19.235+0000] {processor.py:186} INFO - Started process (PID=349) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:42:19.236+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:42:19.238+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:42:19.238+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:42:19.248+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:42:19.265+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:42:19.265+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:42:19.275+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:42:19.275+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:42:19.282+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.053 seconds
[2024-11-17T11:42:49.615+0000] {processor.py:186} INFO - Started process (PID=354) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:42:49.616+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:42:49.618+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:42:49.618+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:42:49.627+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:42:49.643+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:42:49.643+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:42:49.653+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:42:49.653+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:42:49.660+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.049 seconds
[2024-11-17T11:43:19.943+0000] {processor.py:186} INFO - Started process (PID=359) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:43:19.943+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:43:19.945+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:43:19.945+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:43:19.952+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:43:19.967+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:43:19.967+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:43:19.978+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:43:19.978+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:43:19.985+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.046 seconds
[2024-11-17T11:43:50.256+0000] {processor.py:186} INFO - Started process (PID=364) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:43:50.257+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:43:50.259+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:43:50.259+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:43:50.268+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:43:50.284+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:43:50.284+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:43:50.294+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:43:50.294+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:43:50.301+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T11:44:20.622+0000] {processor.py:186} INFO - Started process (PID=374) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:44:20.623+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:44:20.625+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:44:20.625+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:44:20.633+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:44:20.641+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:44:20.641+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:44:20.655+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:44:20.655+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-16T00:00:00+00:00, run_after=2024-11-17T00:00:00+00:00
[2024-11-17T11:44:20.662+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.047 seconds
[2024-11-17T11:44:51.020+0000] {processor.py:186} INFO - Started process (PID=384) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:44:51.021+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:44:51.024+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:44:51.023+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:44:51.034+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:44:51.087+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:44:51.086+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:44:51.095+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:44:51.095+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:44:51.102+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.089 seconds
[2024-11-17T11:45:15.388+0000] {processor.py:186} INFO - Started process (PID=179) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:45:15.388+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:45:15.390+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:45:15.390+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:45:15.399+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:45:15.409+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:45:15.408+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:45:15.424+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:45:15.424+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:45:15.433+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.050 seconds
[2024-11-17T11:45:45.775+0000] {processor.py:186} INFO - Started process (PID=189) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:45:45.776+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:45:45.779+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:45:45.779+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:45:45.792+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:45:45.809+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:45:45.808+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:45:45.821+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:45:45.821+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:45:45.828+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.060 seconds
[2024-11-17T11:46:16.196+0000] {processor.py:186} INFO - Started process (PID=199) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:46:16.196+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:46:16.199+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:46:16.199+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:46:16.206+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:46:16.265+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:46:16.265+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:46:16.272+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:46:16.272+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:46:16.279+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.090 seconds
[2024-11-17T11:46:46.629+0000] {processor.py:186} INFO - Started process (PID=209) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:46:46.630+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:46:46.633+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:46:46.633+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:46:46.644+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:46:46.663+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:46:46.663+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:46:46.678+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:46:46.678+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:46:46.691+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.069 seconds
[2024-11-17T11:47:17.092+0000] {processor.py:186} INFO - Started process (PID=219) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:47:17.092+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:47:17.095+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:47:17.095+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:47:17.104+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:47:17.174+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:47:17.174+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:47:17.182+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:47:17.182+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:47:17.188+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.105 seconds
[2024-11-17T11:47:41.473+0000] {processor.py:186} INFO - Started process (PID=229) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:47:41.473+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:47:41.477+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:47:41.476+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:47:41.489+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:47:41.500+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:47:41.500+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:47:41.512+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:47:41.512+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:47:41.520+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.053 seconds
[2024-11-17T11:48:11.840+0000] {processor.py:186} INFO - Started process (PID=239) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:48:11.841+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:48:11.844+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:48:11.844+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:48:11.855+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:48:11.863+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:48:11.863+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:48:11.877+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:48:11.877+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:48:11.885+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T11:48:42.187+0000] {processor.py:186} INFO - Started process (PID=249) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:48:42.188+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:48:42.191+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:48:42.191+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:48:42.200+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:48:42.255+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:48:42.255+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:48:42.262+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:48:42.262+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:48:42.270+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.091 seconds
[2024-11-17T11:49:12.511+0000] {processor.py:186} INFO - Started process (PID=259) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:49:12.511+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:49:12.514+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:49:12.513+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:49:12.522+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:49:12.543+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:49:12.543+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:49:12.553+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:49:12.553+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:49:12.559+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.054 seconds
[2024-11-17T11:49:42.901+0000] {processor.py:186} INFO - Started process (PID=269) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:49:42.902+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:49:42.905+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:49:42.905+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:49:42.914+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:49:42.925+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:49:42.925+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:49:42.937+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:49:42.937+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:49:42.944+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.049 seconds
[2024-11-17T11:50:22.535+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.535+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 179)
[2024-11-17T11:50:22.536+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.536+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=179
[2024-11-17T11:50:22.539+0000] {processor.py:186} INFO - Started process (PID=179) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:50:22.540+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:50:22.540+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.540+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:50:22.541+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.541+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:50:22.548+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.547+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T11:50:22.548+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:50:22.549+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.549+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T11:50:22.551+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.551+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T11:50:22.559+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.559+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T11:50:22.573+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.573+0000] {serialized_dag.py:182} DEBUG - Writing Serialized DAG: sparkingFlow to the DB
[2024-11-17T11:50:22.575+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.575+0000] {serialized_dag.py:184} DEBUG - DAG: sparkingFlow written to the DB
[2024-11-17T11:50:22.575+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.575+0000] {dagbag.py:745} DEBUG - Syncing DAG permissions: sparkingFlow to the DB
[2024-11-17T11:50:22.612+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.612+0000] {override.py:1134} DEBUG - Not syncing DAG-level permissions for DAG 'sparkingFlow' as access control is unset.
[2024-11-17T11:50:22.613+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.613+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:50:22.621+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:22.621+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:50:22.628+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.094 seconds
[2024-11-17T11:50:52.940+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:52.939+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 189)
[2024-11-17T11:50:52.942+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:52.942+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=189
[2024-11-17T11:50:52.948+0000] {processor.py:186} INFO - Started process (PID=189) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:50:52.949+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:50:52.949+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:52.949+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:50:52.951+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:52.951+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:50:52.961+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:52.961+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T11:50:52.961+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:50:52.961+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:52.961+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T11:50:52.962+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:52.962+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T11:50:52.970+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:52.970+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T11:50:52.986+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:52.986+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T11:50:52.986+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:52.986+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:50:52.997+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:50:52.997+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:50:53.005+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.066 seconds
[2024-11-17T11:51:23.435+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.434+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 199)
[2024-11-17T11:51:23.437+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.436+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=199
[2024-11-17T11:51:23.446+0000] {processor.py:186} INFO - Started process (PID=199) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:51:23.447+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:51:23.448+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.448+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:51:23.450+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.449+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:51:23.458+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.457+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T11:51:23.458+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:51:23.458+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.458+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T11:51:23.458+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.458+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T11:51:23.467+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.467+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T11:51:23.482+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.482+0000] {serialized_dag.py:182} DEBUG - Writing Serialized DAG: sparkingFlow to the DB
[2024-11-17T11:51:23.484+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.484+0000] {serialized_dag.py:184} DEBUG - DAG: sparkingFlow written to the DB
[2024-11-17T11:51:23.484+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.484+0000] {dagbag.py:745} DEBUG - Syncing DAG permissions: sparkingFlow to the DB
[2024-11-17T11:51:23.518+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.518+0000] {override.py:1134} DEBUG - Not syncing DAG-level permissions for DAG 'sparkingFlow' as access control is unset.
[2024-11-17T11:51:23.518+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.518+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:51:23.525+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:23.525+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:51:23.531+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.098 seconds
[2024-11-17T11:51:53.952+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:53.951+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 209)
[2024-11-17T11:51:53.954+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:53.954+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=209
[2024-11-17T11:51:53.960+0000] {processor.py:186} INFO - Started process (PID=209) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:51:53.961+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:51:53.962+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:53.962+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:51:53.964+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:53.964+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:51:53.974+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:53.974+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T11:51:53.978+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:51:53.978+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:53.978+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T11:51:53.978+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:53.978+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T11:51:53.988+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:53.988+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T11:51:54.000+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:54.000+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T11:51:54.003+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:54.003+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:51:54.013+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:51:54.013+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:51:54.022+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.071 seconds
[2024-11-17T11:52:24.462+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.461+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 219)
[2024-11-17T11:52:24.464+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.463+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=219
[2024-11-17T11:52:24.472+0000] {processor.py:186} INFO - Started process (PID=219) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:52:24.473+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:52:24.475+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.475+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:52:24.478+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.478+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:52:24.494+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.493+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T11:52:24.494+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:52:24.494+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.494+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T11:52:24.497+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.497+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T11:52:24.506+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.506+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T11:52:24.515+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.515+0000] {serialized_dag.py:182} DEBUG - Writing Serialized DAG: sparkingFlow to the DB
[2024-11-17T11:52:24.517+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.517+0000] {serialized_dag.py:184} DEBUG - DAG: sparkingFlow written to the DB
[2024-11-17T11:52:24.517+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.517+0000] {dagbag.py:745} DEBUG - Syncing DAG permissions: sparkingFlow to the DB
[2024-11-17T11:52:24.550+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.550+0000] {override.py:1134} DEBUG - Not syncing DAG-level permissions for DAG 'sparkingFlow' as access control is unset.
[2024-11-17T11:52:24.550+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.550+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:52:24.558+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:24.558+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:52:24.564+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.103 seconds
[2024-11-17T11:52:54.948+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:54.947+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 229)
[2024-11-17T11:52:54.949+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:54.949+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=229
[2024-11-17T11:52:54.955+0000] {processor.py:186} INFO - Started process (PID=229) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:52:54.957+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:52:54.957+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:54.957+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:52:54.958+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:54.958+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:52:54.973+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:54.973+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T11:52:54.974+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:52:54.975+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:54.974+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T11:52:54.976+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:54.976+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T11:52:54.984+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:54.984+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T11:52:54.994+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:54.994+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T11:52:54.995+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:54.995+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:52:55.005+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:52:55.005+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:52:55.012+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.065 seconds
[2024-11-17T11:53:25.376+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.376+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 239)
[2024-11-17T11:53:25.377+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.377+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=239
[2024-11-17T11:53:25.384+0000] {processor.py:186} INFO - Started process (PID=239) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:53:25.385+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:53:25.385+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.385+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:53:25.388+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.388+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:53:25.411+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.411+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T11:53:25.412+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:53:25.412+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.412+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T11:53:25.412+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.412+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T11:53:25.421+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.421+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T11:53:25.430+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.430+0000] {serialized_dag.py:182} DEBUG - Writing Serialized DAG: sparkingFlow to the DB
[2024-11-17T11:53:25.432+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.432+0000] {serialized_dag.py:184} DEBUG - DAG: sparkingFlow written to the DB
[2024-11-17T11:53:25.432+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.432+0000] {dagbag.py:745} DEBUG - Syncing DAG permissions: sparkingFlow to the DB
[2024-11-17T11:53:25.472+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.472+0000] {override.py:1134} DEBUG - Not syncing DAG-level permissions for DAG 'sparkingFlow' as access control is unset.
[2024-11-17T11:53:25.472+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.472+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:53:25.479+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:25.479+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:53:25.485+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.110 seconds
[2024-11-17T11:53:55.849+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:55.849+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 249)
[2024-11-17T11:53:55.850+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:55.850+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=249
[2024-11-17T11:53:55.856+0000] {processor.py:186} INFO - Started process (PID=249) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:53:55.856+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:53:55.857+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:55.857+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:53:55.858+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:55.858+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:53:55.865+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:55.865+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T11:53:55.866+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:53:55.866+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:55.866+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T11:53:55.867+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:55.867+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T11:53:55.875+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:55.875+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T11:53:55.883+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:55.883+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T11:53:55.883+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:55.883+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:53:55.894+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:53:55.894+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:53:55.901+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.052 seconds
[2024-11-17T11:54:26.342+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.341+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 259)
[2024-11-17T11:54:26.342+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.342+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=259
[2024-11-17T11:54:26.349+0000] {processor.py:186} INFO - Started process (PID=259) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:54:26.349+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:54:26.350+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.350+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:54:26.351+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.351+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:54:26.358+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.358+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T11:54:26.358+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:54:26.359+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.359+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T11:54:26.359+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.359+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T11:54:26.367+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.367+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T11:54:26.374+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.374+0000] {serialized_dag.py:182} DEBUG - Writing Serialized DAG: sparkingFlow to the DB
[2024-11-17T11:54:26.376+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.376+0000] {serialized_dag.py:184} DEBUG - DAG: sparkingFlow written to the DB
[2024-11-17T11:54:26.376+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.376+0000] {dagbag.py:745} DEBUG - Syncing DAG permissions: sparkingFlow to the DB
[2024-11-17T11:54:26.411+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.411+0000] {override.py:1134} DEBUG - Not syncing DAG-level permissions for DAG 'sparkingFlow' as access control is unset.
[2024-11-17T11:54:26.411+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.411+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:54:26.419+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:26.419+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:54:26.425+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.084 seconds
[2024-11-17T11:54:56.809+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:56.809+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 269)
[2024-11-17T11:54:56.810+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:56.810+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=269
[2024-11-17T11:54:56.815+0000] {processor.py:186} INFO - Started process (PID=269) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:54:56.816+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T11:54:56.817+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:56.817+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:54:56.819+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:56.819+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:54:56.827+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:56.827+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T11:54:56.827+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T11:54:56.827+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:56.827+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T11:54:56.827+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:56.827+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T11:54:56.836+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:56.836+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T11:54:56.843+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:56.843+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T11:54:56.844+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:56.844+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T11:54:56.854+0000] {logging_mixin.py:190} INFO - [2024-11-17T11:54:56.854+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T11:54:56.860+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.052 seconds
[2024-11-17T13:37:42.755+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.755+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 179)
[2024-11-17T13:37:42.756+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.756+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=179
[2024-11-17T13:37:42.761+0000] {processor.py:186} INFO - Started process (PID=179) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:37:42.761+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:37:42.762+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.762+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:37:42.763+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.763+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:37:42.770+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.770+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:37:42.770+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:37:42.771+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.771+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:37:42.772+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.772+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:37:42.777+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.777+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:37:42.792+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.792+0000] {serialized_dag.py:182} DEBUG - Writing Serialized DAG: sparkingFlow to the DB
[2024-11-17T13:37:42.795+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.795+0000] {serialized_dag.py:184} DEBUG - DAG: sparkingFlow written to the DB
[2024-11-17T13:37:42.797+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.797+0000] {dagbag.py:745} DEBUG - Syncing DAG permissions: sparkingFlow to the DB
[2024-11-17T13:37:42.825+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.825+0000] {override.py:1819} ERROR - Add View Menu Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "serialized_dag_pkey"
DETAIL:  Key (dag_id)=(sparkingFlow) already exists.

[SQL: INSERT INTO serialized_dag (dag_id, fileloc, fileloc_hash, data, data_compressed, last_updated, dag_hash, processor_subdir) VALUES (%(dag_id)s, %(fileloc)s, %(fileloc_hash)s, %(data)s, %(data_compressed)s, %(last_updated)s, %(dag_hash)s, %(processor_subdir)s)]
[parameters: {'dag_id': 'sparkingFlow', 'fileloc': '/opt/airflow/dags/spark_airflow.py', 'fileloc_hash': 16732433089539742, 'data': '{"__version": 1, "dag": {"_task_group": {"_group_id": null, "prefix_group_id": true, "tooltip": "", "ui_color": "CornflowerBlue", "ui_fgcolor": "#000 ... (2746 characters truncated) ... hon", "_is_empty": false, "start_trigger_args": null, "op_args": [], "op_kwargs": {}}, "__type": "operator"}], "dag_dependencies": [], "params": []}}', 'data_compressed': None, 'last_updated': datetime.datetime(2024, 11, 17, 13, 37, 42, 777682, tzinfo=Timezone('UTC')), 'dag_hash': '00fb9bed936b456f12444364e470c466', 'processor_subdir': '/opt/airflow/dags'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-11-17T13:37:42.827+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.827+0000] {override.py:1911} INFO - Created Permission View: can edit on None
[2024-11-17T13:37:42.832+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.832+0000] {override.py:1914} ERROR - Creation of Permission View Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "ab_permission_view_permission_id_view_menu_id_uq"
DETAIL:  Key (permission_id, view_menu_id)=(4, 21) already exists.

[SQL: INSERT INTO ab_permission_view (permission_id, view_menu_id) VALUES (%(permission_id)s, %(view_menu_id)s) RETURNING ab_permission_view.id]
[parameters: {'permission_id': 4, 'view_menu_id': 21}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-11-17T13:37:42.834+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.834+0000] {override.py:1914} ERROR - Creation of Permission View Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "ab_permission_view_permission_id_view_menu_id_uq"
DETAIL:  Key (permission_id, view_menu_id)=(3, 22) already exists.

[SQL: INSERT INTO ab_permission_view (permission_id, view_menu_id) VALUES (%(permission_id)s, %(view_menu_id)s) RETURNING ab_permission_view.id]
[parameters: {'permission_id': 3, 'view_menu_id': 22}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-11-17T13:37:42.836+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.836+0000] {override.py:1914} ERROR - Creation of Permission View Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "ab_permission_view_permission_id_view_menu_id_uq"
DETAIL:  Key (permission_id, view_menu_id)=(2, 22) already exists.

[SQL: INSERT INTO ab_permission_view (permission_id, view_menu_id) VALUES (%(permission_id)s, %(view_menu_id)s) RETURNING ab_permission_view.id]
[parameters: {'permission_id': 2, 'view_menu_id': 22}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-11-17T13:37:42.838+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.838+0000] {override.py:1914} ERROR - Creation of Permission View Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "ab_permission_view_permission_id_view_menu_id_uq"
DETAIL:  Key (permission_id, view_menu_id)=(5, 22) already exists.

[SQL: INSERT INTO ab_permission_view (permission_id, view_menu_id) VALUES (%(permission_id)s, %(view_menu_id)s) RETURNING ab_permission_view.id]
[parameters: {'permission_id': 5, 'view_menu_id': 22}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-11-17T13:37:42.839+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.839+0000] {override.py:1134} DEBUG - Not syncing DAG-level permissions for DAG 'sparkingFlow' as access control is unset.
[2024-11-17T13:37:42.842+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.842+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:37:42.847+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.847+0000] {dag.py:3262} INFO - Creating ORM DAG for sparkingFlow
[2024-11-17T13:37:42.850+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.850+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T13:37:42.858+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.853+0000] {before_sleep.py:65} DEBUG - Retrying <unknown> in 0.08828497400648871 seconds as it raised IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(sparkingFlow) already exists.

[SQL: INSERT INTO dag (dag_id, root_dag_id, is_paused, is_subdag, is_active, last_parsed_time, last_pickled, last_expired, scheduler_lock, pickle_id, fileloc, processor_subdir, owners, dag_display_name, description, default_view, schedule_interval, timetable_description, dataset_expression, max_active_tasks, max_active_runs, max_consecutive_failed_dag_runs, has_task_concurrency_limits, has_import_errors, next_dagrun, next_dagrun_data_interval_start, next_dagrun_data_interval_end, next_dagrun_create_after) VALUES (%(dag_id)s, %(root_dag_id)s, %(is_paused)s, %(is_subdag)s, %(is_active)s, %(last_parsed_time)s, %(last_pickled)s, %(last_expired)s, %(scheduler_lock)s, %(pickle_id)s, %(fileloc)s, %(processor_subdir)s, %(owners)s, %(dag_display_name)s, %(description)s, %(default_view)s, %(schedule_interval)s, %(timetable_description)s, %(dataset_expression)s, %(max_active_tasks)s, %(max_active_runs)s, %(max_consecutive_failed_dag_runs)s, %(has_task_concurrency_limits)s, %(has_import_errors)s, %(next_dagrun)s, %(next_dagrun_data_interval_start)s, %(next_dagrun_data_interval_end)s, %(next_dagrun_create_after)s)]
[parameters: {'dag_id': 'sparkingFlow', 'root_dag_id': None, 'is_paused': True, 'is_subdag': False, 'is_active': True, 'last_parsed_time': datetime.datetime(2024, 11, 17, 13, 37, 42, 850850, tzinfo=Timezone('UTC')), 'last_pickled': None, 'last_expired': None, 'scheduler_lock': None, 'pickle_id': None, 'fileloc': '/opt/airflow/dags/spark_airflow.py', 'processor_subdir': '/opt/airflow/dags', 'owners': 'Supakorn', 'dag_display_name': None, 'description': None, 'default_view': 'grid', 'schedule_interval': '{"type": "timedelta", "attrs": {"days": 1, "seconds": 0, "microseconds": 0}}', 'timetable_description': '', 'dataset_expression': 'null', 'max_active_tasks': 16, 'max_active_runs': 16, 'max_consecutive_failed_dag_runs': 0, 'has_task_concurrency_limits': False, 'has_import_errors': False, 'next_dagrun': None, 'next_dagrun_data_interval_start': None, 'next_dagrun_data_interval_end': None, 'next_dagrun_create_after': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj).
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(sparkingFlow) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 724, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 3425, in bulk_write_to_db
    dataset_manager.create_datasets(dataset_models=new_dataset_models, session=session)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/datasets/manager.py", line 65, in create_datasets
    session.flush()
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/persistence.py", line 1097, in _emit_insert_statements
    c = connection._execute_20(
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(sparkingFlow) already exists.

[SQL: INSERT INTO dag (dag_id, root_dag_id, is_paused, is_subdag, is_active, last_parsed_time, last_pickled, last_expired, scheduler_lock, pickle_id, fileloc, processor_subdir, owners, dag_display_name, description, default_view, schedule_interval, timetable_description, dataset_expression, max_active_tasks, max_active_runs, max_consecutive_failed_dag_runs, has_task_concurrency_limits, has_import_errors, next_dagrun, next_dagrun_data_interval_start, next_dagrun_data_interval_end, next_dagrun_create_after) VALUES (%(dag_id)s, %(root_dag_id)s, %(is_paused)s, %(is_subdag)s, %(is_active)s, %(last_parsed_time)s, %(last_pickled)s, %(last_expired)s, %(scheduler_lock)s, %(pickle_id)s, %(fileloc)s, %(processor_subdir)s, %(owners)s, %(dag_display_name)s, %(description)s, %(default_view)s, %(schedule_interval)s, %(timetable_description)s, %(dataset_expression)s, %(max_active_tasks)s, %(max_active_runs)s, %(max_consecutive_failed_dag_runs)s, %(has_task_concurrency_limits)s, %(has_import_errors)s, %(next_dagrun)s, %(next_dagrun_data_interval_start)s, %(next_dagrun_data_interval_end)s, %(next_dagrun_create_after)s)]
[parameters: {'dag_id': 'sparkingFlow', 'root_dag_id': None, 'is_paused': True, 'is_subdag': False, 'is_active': True, 'last_parsed_time': datetime.datetime(2024, 11, 17, 13, 37, 42, 850850, tzinfo=Timezone('UTC')), 'last_pickled': None, 'last_expired': None, 'scheduler_lock': None, 'pickle_id': None, 'fileloc': '/opt/airflow/dags/spark_airflow.py', 'processor_subdir': '/opt/airflow/dags', 'owners': 'Supakorn', 'dag_display_name': None, 'description': None, 'default_view': 'grid', 'schedule_interval': '{"type": "timedelta", "attrs": {"days": 1, "seconds": 0, "microseconds": 0}}', 'timetable_description': '', 'dataset_expression': 'null', 'max_active_tasks': 16, 'max_active_runs': 16, 'max_consecutive_failed_dag_runs': 0, 'has_task_concurrency_limits': False, 'has_import_errors': False, 'next_dagrun': None, 'next_dagrun_data_interval_start': None, 'next_dagrun_data_interval_end': None, 'next_dagrun_create_after': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-11-17T13:37:42.948+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.948+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 2 of 3
[2024-11-17T13:37:42.949+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.949+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:37:42.950+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.949+0000] {dagbag.py:698} ERROR - Failed to write serialized DAG: /opt/airflow/dags/spark_airflow.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 686, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/serialized_dag.py", line 160, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(sparkingFlow) already exists.

[SQL: INSERT INTO dag (dag_id, root_dag_id, is_paused, is_subdag, is_active, last_parsed_time, last_pickled, last_expired, scheduler_lock, pickle_id, fileloc, processor_subdir, owners, dag_display_name, description, default_view, schedule_interval, timetable_description, dataset_expression, max_active_tasks, max_active_runs, max_consecutive_failed_dag_runs, has_task_concurrency_limits, has_import_errors, next_dagrun, next_dagrun_data_interval_start, next_dagrun_data_interval_end, next_dagrun_create_after) VALUES (%(dag_id)s, %(root_dag_id)s, %(is_paused)s, %(is_subdag)s, %(is_active)s, %(last_parsed_time)s, %(last_pickled)s, %(last_expired)s, %(scheduler_lock)s, %(pickle_id)s, %(fileloc)s, %(processor_subdir)s, %(owners)s, %(dag_display_name)s, %(description)s, %(default_view)s, %(schedule_interval)s, %(timetable_description)s, %(dataset_expression)s, %(max_active_tasks)s, %(max_active_runs)s, %(max_consecutive_failed_dag_runs)s, %(has_task_concurrency_limits)s, %(has_import_errors)s, %(next_dagrun)s, %(next_dagrun_data_interval_start)s, %(next_dagrun_data_interval_end)s, %(next_dagrun_create_after)s)]
[parameters: {'dag_id': 'sparkingFlow', 'root_dag_id': None, 'is_paused': True, 'is_subdag': False, 'is_active': True, 'last_parsed_time': datetime.datetime(2024, 11, 17, 13, 37, 42, 850850, tzinfo=Timezone('UTC')), 'last_pickled': None, 'last_expired': None, 'scheduler_lock': None, 'pickle_id': None, 'fileloc': '/opt/airflow/dags/spark_airflow.py', 'processor_subdir': '/opt/airflow/dags', 'owners': 'Supakorn', 'dag_display_name': None, 'description': None, 'default_view': 'grid', 'schedule_interval': '{"type": "timedelta", "attrs": {"days": 1, "seconds": 0, "microseconds": 0}}', 'timetable_description': '', 'dataset_expression': 'null', 'max_active_tasks': 16, 'max_active_runs': 16, 'max_consecutive_failed_dag_runs': 0, 'has_task_concurrency_limits': False, 'has_import_errors': False, 'next_dagrun': None, 'next_dagrun_data_interval_start': None, 'next_dagrun_data_interval_end': None, 'next_dagrun_create_after': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-11-17T13:37:42.951+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:37:42.951+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:37:42.951+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/api_internal/internal_api_call.py", line 166, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 708, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 724, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 3252, in bulk_write_to_db
    orm_dags: list[DagModel] = session.scalars(query).unique().all()
                               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 1778, in scalars
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 724, in _connection_for_bind
    self._assert_active()
  File "/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dag_pkey"
DETAIL:  Key (dag_id)=(sparkingFlow) already exists.

[SQL: INSERT INTO dag (dag_id, root_dag_id, is_paused, is_subdag, is_active, last_parsed_time, last_pickled, last_expired, scheduler_lock, pickle_id, fileloc, processor_subdir, owners, dag_display_name, description, default_view, schedule_interval, timetable_description, dataset_expression, max_active_tasks, max_active_runs, max_consecutive_failed_dag_runs, has_task_concurrency_limits, has_import_errors, next_dagrun, next_dagrun_data_interval_start, next_dagrun_data_interval_end, next_dagrun_create_after) VALUES (%(dag_id)s, %(root_dag_id)s, %(is_paused)s, %(is_subdag)s, %(is_active)s, %(last_parsed_time)s, %(last_pickled)s, %(last_expired)s, %(scheduler_lock)s, %(pickle_id)s, %(fileloc)s, %(processor_subdir)s, %(owners)s, %(dag_display_name)s, %(description)s, %(default_view)s, %(schedule_interval)s, %(timetable_description)s, %(dataset_expression)s, %(max_active_tasks)s, %(max_active_runs)s, %(max_consecutive_failed_dag_runs)s, %(has_task_concurrency_limits)s, %(has_import_errors)s, %(next_dagrun)s, %(next_dagrun_data_interval_start)s, %(next_dagrun_data_interval_end)s, %(next_dagrun_create_after)s)]
[parameters: {'dag_id': 'sparkingFlow', 'root_dag_id': None, 'is_paused': True, 'is_subdag': False, 'is_active': True, 'last_parsed_time': datetime.datetime(2024, 11, 17, 13, 37, 42, 850850, tzinfo=Timezone('UTC')), 'last_pickled': None, 'last_expired': None, 'scheduler_lock': None, 'pickle_id': None, 'fileloc': '/opt/airflow/dags/spark_airflow.py', 'processor_subdir': '/opt/airflow/dags', 'owners': 'Supakorn', 'dag_display_name': None, 'description': None, 'default_view': 'grid', 'schedule_interval': '{"type": "timedelta", "attrs": {"days": 1, "seconds": 0, "microseconds": 0}}', 'timetable_description': '', 'dataset_expression': 'null', 'max_active_tasks': 16, 'max_active_runs': 16, 'max_consecutive_failed_dag_runs': 0, 'has_task_concurrency_limits': False, 'has_import_errors': False, 'next_dagrun': None, 'next_dagrun_data_interval_start': None, 'next_dagrun_data_interval_end': None, 'next_dagrun_create_after': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2024-11-17T13:38:13.238+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.238+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 189)
[2024-11-17T13:38:13.239+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.239+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=189
[2024-11-17T13:38:13.243+0000] {processor.py:186} INFO - Started process (PID=189) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:38:13.244+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:38:13.244+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.244+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:38:13.245+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.245+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:38:13.253+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.253+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:38:13.255+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:38:13.255+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.255+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:38:13.255+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.255+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:38:13.263+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.263+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:38:13.274+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.274+0000] {serialized_dag.py:182} DEBUG - Writing Serialized DAG: sparkingFlow to the DB
[2024-11-17T13:38:13.276+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.276+0000] {serialized_dag.py:184} DEBUG - DAG: sparkingFlow written to the DB
[2024-11-17T13:38:13.278+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.278+0000] {dagbag.py:745} DEBUG - Syncing DAG permissions: sparkingFlow to the DB
[2024-11-17T13:38:13.310+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.310+0000] {override.py:1134} DEBUG - Not syncing DAG-level permissions for DAG 'sparkingFlow' as access control is unset.
[2024-11-17T13:38:13.310+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.310+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:38:13.317+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:13.317+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T13:38:13.323+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.086 seconds
[2024-11-17T13:38:43.627+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:43.627+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 199)
[2024-11-17T13:38:43.628+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:43.628+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=199
[2024-11-17T13:38:43.636+0000] {processor.py:186} INFO - Started process (PID=199) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:38:43.637+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:38:43.637+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:43.637+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:38:43.639+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:43.639+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:38:43.647+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:43.647+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:38:43.647+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:38:43.647+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:43.647+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:38:43.648+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:43.648+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:38:43.656+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:43.656+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:38:43.667+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:43.667+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:38:43.668+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:43.668+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:38:43.681+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:38:43.681+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to None, run_after=None
[2024-11-17T13:38:43.687+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.060 seconds
[2024-11-17T13:39:14.060+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.060+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 312)
[2024-11-17T13:39:14.061+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.061+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=312
[2024-11-17T13:39:14.067+0000] {processor.py:186} INFO - Started process (PID=312) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:39:14.068+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:39:14.068+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.068+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:39:14.070+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.070+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:39:14.080+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.080+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:39:14.083+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:39:14.084+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.084+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:39:14.084+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.084+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:39:14.093+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.093+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:39:14.105+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.105+0000] {serialized_dag.py:182} DEBUG - Writing Serialized DAG: sparkingFlow to the DB
[2024-11-17T13:39:14.107+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.107+0000] {serialized_dag.py:184} DEBUG - DAG: sparkingFlow written to the DB
[2024-11-17T13:39:14.110+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.110+0000] {dagbag.py:745} DEBUG - Syncing DAG permissions: sparkingFlow to the DB
[2024-11-17T13:39:14.145+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.145+0000] {override.py:1134} DEBUG - Not syncing DAG-level permissions for DAG 'sparkingFlow' as access control is unset.
[2024-11-17T13:39:14.145+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.145+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:39:14.153+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:14.153+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:39:14.159+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.099 seconds
[2024-11-17T13:39:44.538+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:44.537+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 322)
[2024-11-17T13:39:44.538+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:44.538+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=322
[2024-11-17T13:39:44.543+0000] {processor.py:186} INFO - Started process (PID=322) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:39:44.544+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:39:44.544+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:44.544+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:39:44.546+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:44.545+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:39:44.560+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:44.560+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:39:44.561+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:39:44.561+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:44.561+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:39:44.561+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:44.561+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:39:44.570+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:44.570+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:39:44.576+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:44.576+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:39:44.577+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:44.577+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:39:44.586+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:39:44.586+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:39:44.592+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.055 seconds
[2024-11-17T13:40:14.919+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:14.918+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 327)
[2024-11-17T13:40:14.920+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:14.919+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=327
[2024-11-17T13:40:14.924+0000] {processor.py:186} INFO - Started process (PID=327) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:40:14.924+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:40:14.925+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:14.925+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:40:14.926+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:14.926+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:40:14.942+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:14.942+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:40:14.942+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:40:14.942+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:14.942+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:40:14.943+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:14.943+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:40:14.949+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:14.949+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:40:14.956+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:14.956+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:40:14.956+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:14.956+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:40:14.965+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:14.965+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:40:14.971+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.053 seconds
[2024-11-17T13:40:45.297+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:45.297+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 332)
[2024-11-17T13:40:45.298+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:45.298+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=332
[2024-11-17T13:40:45.303+0000] {processor.py:186} INFO - Started process (PID=332) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:40:45.303+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:40:45.304+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:45.304+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:40:45.305+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:45.305+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:40:45.320+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:45.319+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:40:45.320+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:40:45.320+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:45.320+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:40:45.320+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:45.320+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:40:45.329+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:45.329+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:40:45.335+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:45.335+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:40:45.335+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:45.335+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:40:45.344+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:40:45.344+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:40:45.350+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.054 seconds
[2024-11-17T13:41:15.712+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:15.711+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 337)
[2024-11-17T13:41:15.714+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:15.714+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=337
[2024-11-17T13:41:15.721+0000] {processor.py:186} INFO - Started process (PID=337) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:41:15.721+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:41:15.722+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:15.722+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:41:15.723+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:15.723+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:41:15.745+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:15.744+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:41:15.745+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:41:15.745+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:15.745+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:41:15.745+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:15.745+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:41:15.753+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:15.753+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:41:15.760+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:15.760+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:41:15.760+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:15.760+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:41:15.771+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:15.771+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:41:15.777+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.066 seconds
[2024-11-17T13:41:46.131+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:46.130+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 342)
[2024-11-17T13:41:46.132+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:46.132+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=342
[2024-11-17T13:41:46.137+0000] {processor.py:186} INFO - Started process (PID=342) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:41:46.138+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:41:46.138+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:46.138+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:41:46.140+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:46.140+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:41:46.156+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:46.155+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:41:46.156+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:41:46.156+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:46.156+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:41:46.157+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:46.157+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:41:46.164+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:46.164+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:41:46.171+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:46.171+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:41:46.171+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:46.171+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:41:46.181+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:41:46.181+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:41:46.188+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.058 seconds
[2024-11-17T13:42:16.583+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:16.582+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 347)
[2024-11-17T13:42:16.584+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:16.584+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=347
[2024-11-17T13:42:16.589+0000] {processor.py:186} INFO - Started process (PID=347) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:42:16.590+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:42:16.590+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:16.590+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:42:16.592+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:16.592+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:42:16.608+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:16.608+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:42:16.608+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:42:16.609+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:16.609+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:42:16.609+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:16.609+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:42:16.618+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:16.618+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:42:16.625+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:16.625+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:42:16.625+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:16.625+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:42:16.635+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:16.635+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:42:16.642+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.060 seconds
[2024-11-17T13:42:46.989+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:46.988+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 352)
[2024-11-17T13:42:46.990+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:46.990+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=352
[2024-11-17T13:42:46.995+0000] {processor.py:186} INFO - Started process (PID=352) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:42:46.996+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:42:46.996+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:46.996+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:42:46.997+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:46.997+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:42:47.006+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:47.005+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:42:47.006+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:42:47.006+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:47.006+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:42:47.006+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:47.006+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:42:47.013+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:47.013+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:42:47.020+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:47.020+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:42:47.020+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:47.020+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:42:47.029+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:42:47.029+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:42:47.036+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.048 seconds
[2024-11-17T13:43:17.343+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:17.343+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 357)
[2024-11-17T13:43:17.344+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:17.344+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=357
[2024-11-17T13:43:17.349+0000] {processor.py:186} INFO - Started process (PID=357) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:43:17.350+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:43:17.350+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:17.350+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:43:17.352+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:17.352+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:43:17.363+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:17.362+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:43:17.363+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:43:17.363+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:17.363+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:43:17.363+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:17.363+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:43:17.371+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:17.371+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:43:17.379+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:17.379+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:43:17.380+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:17.380+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:43:17.390+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:17.390+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:43:17.397+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.054 seconds
[2024-11-17T13:43:47.681+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:47.680+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 362)
[2024-11-17T13:43:47.681+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:47.681+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=362
[2024-11-17T13:43:47.685+0000] {processor.py:186} INFO - Started process (PID=362) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:43:47.686+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:43:47.686+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:47.686+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:43:47.687+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:47.687+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:43:47.693+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:47.693+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:43:47.693+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:43:47.693+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:47.693+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:43:47.694+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:47.693+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:43:47.701+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:47.700+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:43:47.707+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:47.707+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:43:47.707+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:47.707+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:43:47.717+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:43:47.717+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:43:47.723+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.043 seconds
[2024-11-17T13:44:18.018+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:18.018+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 419)
[2024-11-17T13:44:18.019+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:18.019+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=419
[2024-11-17T13:44:18.024+0000] {processor.py:186} INFO - Started process (PID=419) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:44:18.024+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:44:18.025+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:18.025+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:44:18.026+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:18.026+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:44:18.035+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:18.035+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:44:18.035+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:44:18.036+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:18.036+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:44:18.036+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:18.036+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:44:18.044+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:18.044+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:44:18.052+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:18.052+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:44:18.052+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:18.052+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:44:18.062+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:18.062+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:44:18.070+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.053 seconds
[2024-11-17T13:44:48.432+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:48.432+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 424)
[2024-11-17T13:44:48.433+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:48.433+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=424
[2024-11-17T13:44:48.438+0000] {processor.py:186} INFO - Started process (PID=424) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:44:48.438+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:44:48.439+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:48.439+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:44:48.440+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:48.440+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:44:48.449+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:48.448+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:44:48.449+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:44:48.449+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:48.449+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:44:48.449+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:48.449+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:44:48.458+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:48.458+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:44:48.465+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:48.465+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:44:48.465+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:48.465+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:44:48.474+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:44:48.474+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:44:48.480+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.049 seconds
[2024-11-17T13:45:18.832+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:18.831+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 429)
[2024-11-17T13:45:18.833+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:18.833+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=429
[2024-11-17T13:45:18.838+0000] {processor.py:186} INFO - Started process (PID=429) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:45:18.838+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:45:18.839+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:18.839+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:45:18.840+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:18.840+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:45:18.848+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:18.848+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:45:18.849+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:45:18.849+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:18.849+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:45:18.849+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:18.849+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:45:18.857+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:18.857+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:45:18.864+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:18.864+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:45:18.864+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:18.864+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:45:18.873+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:18.873+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:45:18.880+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.049 seconds
[2024-11-17T13:45:49.240+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:49.239+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 486)
[2024-11-17T13:45:49.240+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:49.240+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=486
[2024-11-17T13:45:49.245+0000] {processor.py:186} INFO - Started process (PID=486) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:45:49.245+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:45:49.246+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:49.246+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:45:49.247+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:49.247+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:45:49.256+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:49.256+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:45:49.256+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:45:49.257+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:49.257+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:45:49.257+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:49.257+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:45:49.266+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:49.266+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:45:49.273+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:49.273+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:45:49.273+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:49.273+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:45:49.282+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:45:49.282+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:45:49.289+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.050 seconds
[2024-11-17T13:46:19.657+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:19.657+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 491)
[2024-11-17T13:46:19.658+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:19.658+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=491
[2024-11-17T13:46:19.662+0000] {processor.py:186} INFO - Started process (PID=491) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:46:19.662+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:46:19.663+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:19.663+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:46:19.664+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:19.664+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:46:19.671+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:19.670+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:46:19.671+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:46:19.671+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:19.671+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:46:19.671+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:19.671+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:46:19.679+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:19.679+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:46:19.687+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:19.687+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:46:19.687+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:19.687+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:46:19.701+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:19.701+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:46:19.708+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.052 seconds
[2024-11-17T13:46:50.054+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:50.054+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 496)
[2024-11-17T13:46:50.055+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:50.055+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=496
[2024-11-17T13:46:50.060+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:46:50.060+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:46:50.061+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:50.061+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:46:50.062+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:50.062+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:46:50.070+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:50.070+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:46:50.070+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:46:50.071+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:50.071+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:46:50.071+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:50.071+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:46:50.079+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:50.079+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:46:50.085+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:50.085+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:46:50.086+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:50.085+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:46:50.095+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:46:50.095+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:46:50.101+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.047 seconds
[2024-11-17T13:47:20.453+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:20.452+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 501)
[2024-11-17T13:47:20.454+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:20.453+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=501
[2024-11-17T13:47:20.459+0000] {processor.py:186} INFO - Started process (PID=501) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:47:20.459+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:47:20.460+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:20.460+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:47:20.462+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:20.462+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:47:20.472+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:20.472+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:47:20.472+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:47:20.472+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:20.472+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:47:20.473+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:20.473+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:47:20.483+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:20.483+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:47:20.490+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:20.490+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:47:20.490+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:20.490+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:47:20.501+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:20.501+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:47:20.508+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.056 seconds
[2024-11-17T13:47:50.839+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:50.838+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 506)
[2024-11-17T13:47:50.840+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:50.840+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=506
[2024-11-17T13:47:50.843+0000] {processor.py:186} INFO - Started process (PID=506) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:47:50.844+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:47:50.844+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:50.844+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:47:50.845+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:50.845+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:47:50.852+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:50.852+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:47:50.852+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:47:50.853+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:50.853+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:47:50.853+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:50.853+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:47:50.861+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:50.861+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:47:50.869+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:50.869+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:47:50.869+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:50.869+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:47:50.883+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:47:50.882+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:47:50.890+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.052 seconds
[2024-11-17T13:48:21.212+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:21.212+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 511)
[2024-11-17T13:48:21.213+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:21.213+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=511
[2024-11-17T13:48:21.217+0000] {processor.py:186} INFO - Started process (PID=511) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:48:21.217+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:48:21.218+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:21.217+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:48:21.218+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:21.218+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:48:21.225+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:21.224+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:48:21.225+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:48:21.225+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:21.225+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:48:21.225+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:21.225+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:48:21.232+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:21.232+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:48:21.239+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:21.238+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:48:21.239+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:21.239+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:48:21.248+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:21.248+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:48:21.254+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.043 seconds
[2024-11-17T13:48:51.612+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:51.611+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 516)
[2024-11-17T13:48:51.613+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:51.613+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=516
[2024-11-17T13:48:51.618+0000] {processor.py:186} INFO - Started process (PID=516) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:48:51.618+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:48:51.619+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:51.618+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:48:51.620+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:51.620+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:48:51.628+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:51.627+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:48:51.628+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:48:51.628+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:51.628+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:48:51.628+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:51.628+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:48:51.636+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:51.636+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:48:51.643+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:51.643+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:48:51.643+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:51.643+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:48:51.652+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:48:51.652+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:48:51.658+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.047 seconds
[2024-11-17T13:49:21.980+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:21.979+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 521)
[2024-11-17T13:49:21.981+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:21.981+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=521
[2024-11-17T13:49:21.985+0000] {processor.py:186} INFO - Started process (PID=521) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:49:21.986+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:49:21.987+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:21.986+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:49:21.988+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:21.988+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:49:21.997+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:21.996+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:49:21.997+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:49:21.997+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:21.997+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:49:21.997+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:21.997+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:49:22.006+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:22.006+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:49:22.013+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:22.013+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:49:22.013+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:22.013+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:49:22.023+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:22.023+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:49:22.030+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.050 seconds
[2024-11-17T13:49:52.364+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:52.363+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 526)
[2024-11-17T13:49:52.364+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:52.364+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=526
[2024-11-17T13:49:52.368+0000] {processor.py:186} INFO - Started process (PID=526) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:49:52.368+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:49:52.369+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:52.369+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:49:52.370+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:52.370+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:49:52.377+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:52.377+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:49:52.377+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:49:52.378+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:52.378+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:49:52.378+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:52.378+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:49:52.385+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:52.385+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:49:52.393+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:52.393+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:49:52.393+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:52.393+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:49:52.405+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:49:52.405+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:49:52.412+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.049 seconds
[2024-11-17T13:50:22.762+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:22.762+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 531)
[2024-11-17T13:50:22.763+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:22.763+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=531
[2024-11-17T13:50:22.768+0000] {processor.py:186} INFO - Started process (PID=531) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:50:22.768+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:50:22.769+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:22.769+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:50:22.770+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:22.770+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:50:22.779+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:22.779+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:50:22.779+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:50:22.779+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:22.779+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:50:22.779+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:22.779+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:50:22.788+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:22.788+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:50:22.796+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:22.796+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:50:22.796+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:22.796+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:50:22.808+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:22.808+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:50:22.815+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.053 seconds
[2024-11-17T13:50:53.156+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:53.155+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 636)
[2024-11-17T13:50:53.156+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:53.156+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=636
[2024-11-17T13:50:53.161+0000] {processor.py:186} INFO - Started process (PID=636) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:50:53.162+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:50:53.162+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:53.162+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:50:53.163+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:53.163+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:50:53.172+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:53.171+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:50:53.172+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:50:53.172+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:53.172+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:50:53.172+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:53.172+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:50:53.181+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:53.180+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:50:53.187+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:53.187+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:50:53.188+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:53.187+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:50:53.197+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:50:53.197+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:50:53.203+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.048 seconds
[2024-11-17T13:51:23.559+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:23.559+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 641)
[2024-11-17T13:51:23.561+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:23.561+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=641
[2024-11-17T13:51:23.566+0000] {processor.py:186} INFO - Started process (PID=641) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:51:23.567+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:51:23.567+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:23.567+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:51:23.569+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:23.569+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:51:23.578+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:23.578+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:51:23.578+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:51:23.579+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:23.579+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:51:23.579+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:23.579+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:51:23.587+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:23.587+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:51:23.595+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:23.595+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:51:23.595+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:23.595+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:51:23.604+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:23.604+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:51:23.611+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.053 seconds
[2024-11-17T13:51:53.966+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:53.966+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 646)
[2024-11-17T13:51:53.967+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:53.967+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=646
[2024-11-17T13:51:53.971+0000] {processor.py:186} INFO - Started process (PID=646) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:51:53.972+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:51:53.973+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:53.972+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:51:53.974+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:53.974+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:51:53.982+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:53.982+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:51:53.982+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:51:53.983+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:53.983+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:51:53.983+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:53.983+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:51:53.991+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:53.991+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:51:53.999+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:53.999+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:51:53.999+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:53.999+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:51:54.009+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:51:54.009+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:51:54.017+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T13:52:24.324+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:24.324+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 651)
[2024-11-17T13:52:24.325+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:24.325+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=651
[2024-11-17T13:52:24.330+0000] {processor.py:186} INFO - Started process (PID=651) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:52:24.331+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:52:24.332+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:24.332+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:52:24.333+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:24.333+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:52:24.341+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:24.340+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:52:24.341+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:52:24.341+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:24.341+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:52:24.341+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:24.341+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:52:24.350+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:24.349+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:52:24.357+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:24.357+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:52:24.357+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:24.357+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:52:24.367+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:24.367+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:52:24.374+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T13:52:54.679+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:54.678+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 656)
[2024-11-17T13:52:54.680+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:54.680+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=656
[2024-11-17T13:52:54.685+0000] {processor.py:186} INFO - Started process (PID=656) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:52:54.685+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:52:54.686+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:54.686+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:52:54.687+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:54.687+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:52:54.695+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:54.694+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:52:54.695+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:52:54.695+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:54.695+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:52:54.696+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:54.696+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:52:54.703+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:54.703+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:52:54.710+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:54.710+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:52:54.710+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:54.710+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:52:54.720+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:52:54.720+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:52:54.727+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.048 seconds
[2024-11-17T13:53:25.021+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:25.020+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 661)
[2024-11-17T13:53:25.021+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:25.021+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=661
[2024-11-17T13:53:25.025+0000] {processor.py:186} INFO - Started process (PID=661) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:53:25.025+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:53:25.026+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:25.026+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:53:25.026+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:25.026+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:53:25.034+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:25.034+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:53:25.035+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:53:25.035+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:25.035+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:53:25.035+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:25.035+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:53:25.042+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:25.042+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:53:25.050+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:25.050+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:53:25.050+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:25.050+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:53:25.061+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:25.061+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:53:25.071+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.050 seconds
[2024-11-17T13:53:55.387+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:55.387+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 666)
[2024-11-17T13:53:55.388+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:55.388+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=666
[2024-11-17T13:53:55.392+0000] {processor.py:186} INFO - Started process (PID=666) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:53:55.392+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:53:55.393+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:55.393+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:53:55.394+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:55.394+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:53:55.402+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:55.401+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:53:55.402+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:53:55.402+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:55.402+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:53:55.402+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:55.402+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:53:55.410+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:55.410+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:53:55.419+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:55.419+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:53:55.420+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:55.420+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:53:55.430+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:53:55.429+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:53:55.438+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T13:54:25.828+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:25.828+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 671)
[2024-11-17T13:54:25.829+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:25.829+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=671
[2024-11-17T13:54:25.834+0000] {processor.py:186} INFO - Started process (PID=671) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:54:25.835+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:54:25.835+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:25.835+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:54:25.836+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:25.836+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:54:25.844+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:25.844+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:54:25.844+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:54:25.845+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:25.845+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:54:25.845+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:25.845+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:54:25.853+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:25.853+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:54:25.861+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:25.861+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:54:25.861+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:25.861+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:54:25.872+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:25.872+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:54:25.878+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T13:54:56.237+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:56.236+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 676)
[2024-11-17T13:54:56.238+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:56.238+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=676
[2024-11-17T13:54:56.242+0000] {processor.py:186} INFO - Started process (PID=676) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:54:56.243+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:54:56.243+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:56.243+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:54:56.244+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:56.244+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:54:56.251+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:56.251+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:54:56.251+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:54:56.251+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:56.251+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:54:56.252+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:56.252+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:54:56.260+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:56.260+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:54:56.269+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:56.268+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:54:56.269+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:56.269+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:54:56.280+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:54:56.280+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:54:56.287+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T13:55:26.634+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:26.633+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 681)
[2024-11-17T13:55:26.635+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:26.635+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=681
[2024-11-17T13:55:26.641+0000] {processor.py:186} INFO - Started process (PID=681) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:55:26.641+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:55:26.642+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:26.642+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:55:26.643+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:26.643+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:55:26.652+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:26.652+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:55:26.652+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:55:26.652+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:26.652+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:55:26.652+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:26.652+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:55:26.662+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:26.662+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:55:26.669+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:26.669+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:55:26.669+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:26.669+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:55:26.678+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:26.678+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:55:26.684+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T13:55:57.025+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:57.024+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 686)
[2024-11-17T13:55:57.025+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:57.025+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=686
[2024-11-17T13:55:57.030+0000] {processor.py:186} INFO - Started process (PID=686) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:55:57.032+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:55:57.032+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:57.032+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:55:57.034+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:57.034+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:55:57.043+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:57.042+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:55:57.043+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:55:57.043+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:57.043+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:55:57.043+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:57.043+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:55:57.053+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:57.053+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:55:57.060+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:57.060+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:55:57.060+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:57.060+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:55:57.069+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:55:57.069+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:55:57.076+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.052 seconds
[2024-11-17T13:56:27.429+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:27.428+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 805)
[2024-11-17T13:56:27.430+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:27.430+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=805
[2024-11-17T13:56:27.435+0000] {processor.py:186} INFO - Started process (PID=805) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:56:27.436+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:56:27.437+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:27.436+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:56:27.438+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:27.438+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:56:27.448+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:27.448+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:56:27.449+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:56:27.449+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:27.449+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:56:27.449+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:27.449+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:56:27.458+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:27.458+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:56:27.465+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:27.464+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:56:27.465+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:27.465+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:56:27.474+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:27.474+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:56:27.482+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.054 seconds
[2024-11-17T13:56:57.773+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:57.773+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 816)
[2024-11-17T13:56:57.774+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:57.774+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=816
[2024-11-17T13:56:57.778+0000] {processor.py:186} INFO - Started process (PID=816) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:56:57.778+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:56:57.778+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:57.778+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:56:57.780+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:57.779+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:56:57.787+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:57.786+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:56:57.787+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:56:57.787+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:57.787+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:56:57.787+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:57.787+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:56:57.794+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:57.794+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:56:57.801+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:57.801+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:56:57.801+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:57.801+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:56:57.812+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:56:57.812+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:56:57.818+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.045 seconds
[2024-11-17T13:57:28.149+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:28.148+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 873)
[2024-11-17T13:57:28.150+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:28.149+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=873
[2024-11-17T13:57:28.154+0000] {processor.py:186} INFO - Started process (PID=873) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:57:28.154+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:57:28.155+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:28.154+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:57:28.155+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:28.155+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:57:28.163+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:28.162+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:57:28.163+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:57:28.163+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:28.163+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:57:28.163+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:28.163+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:57:28.170+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:28.170+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:57:28.178+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:28.178+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:57:28.178+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:28.178+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:57:28.188+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:28.188+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:57:28.195+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.047 seconds
[2024-11-17T13:57:58.519+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:58.519+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 878)
[2024-11-17T13:57:58.521+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:58.521+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=878
[2024-11-17T13:57:58.525+0000] {processor.py:186} INFO - Started process (PID=878) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:57:58.525+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:57:58.526+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:58.526+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:57:58.527+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:58.527+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:57:58.535+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:58.535+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:57:58.535+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:57:58.536+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:58.536+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:57:58.536+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:58.536+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:57:58.544+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:58.544+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:57:58.551+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:58.551+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:57:58.551+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:58.551+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:57:58.560+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:57:58.560+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:57:58.567+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.048 seconds
[2024-11-17T13:58:28.875+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:28.875+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 883)
[2024-11-17T13:58:28.876+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:28.876+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=883
[2024-11-17T13:58:28.885+0000] {processor.py:186} INFO - Started process (PID=883) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:58:28.885+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:58:28.886+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:28.886+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:58:28.888+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:28.888+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:58:28.898+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:28.897+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:58:28.898+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:58:28.899+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:28.899+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:58:28.899+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:28.899+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:58:28.908+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:28.908+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:58:28.917+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:28.917+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:58:28.917+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:28.917+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:58:28.927+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:28.927+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:58:28.935+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.061 seconds
[2024-11-17T13:58:59.305+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:59.304+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 888)
[2024-11-17T13:58:59.306+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:59.305+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=888
[2024-11-17T13:58:59.311+0000] {processor.py:186} INFO - Started process (PID=888) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:58:59.311+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:58:59.312+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:59.312+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:58:59.313+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:59.313+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:58:59.322+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:59.321+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:58:59.322+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:58:59.322+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:59.322+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:58:59.322+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:59.322+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:58:59.331+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:59.331+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:58:59.338+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:59.338+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:58:59.338+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:59.338+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:58:59.348+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:58:59.348+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:58:59.355+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T13:59:29.668+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:59:29.667+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 1061)
[2024-11-17T13:59:29.669+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:59:29.669+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1061
[2024-11-17T13:59:29.674+0000] {processor.py:186} INFO - Started process (PID=1061) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:59:29.674+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T13:59:29.675+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:59:29.675+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:59:29.676+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:59:29.676+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:59:29.685+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:59:29.685+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T13:59:29.685+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T13:59:29.686+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:59:29.686+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T13:59:29.686+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:59:29.686+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T13:59:29.693+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:59:29.693+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T13:59:29.701+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:59:29.701+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T13:59:29.701+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:59:29.701+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T13:59:29.712+0000] {logging_mixin.py:190} INFO - [2024-11-17T13:59:29.712+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T13:59:29.718+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T14:00:00.043+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:00.043+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 1240)
[2024-11-17T14:00:00.044+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:00.044+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1240
[2024-11-17T14:00:00.049+0000] {processor.py:186} INFO - Started process (PID=1240) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:00:00.049+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:00:00.050+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:00.050+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:00:00.051+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:00.051+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:00:00.060+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:00.059+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:00:00.060+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:00:00.060+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:00.060+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:00:00.060+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:00.060+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:00:00.068+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:00.068+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:00:00.076+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:00.076+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:00:00.076+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:00.076+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:00:00.086+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:00.086+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T14:00:00.093+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.051 seconds
[2024-11-17T14:00:30.457+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:30.456+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 1245)
[2024-11-17T14:00:30.458+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:30.458+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1245
[2024-11-17T14:00:30.463+0000] {processor.py:186} INFO - Started process (PID=1245) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:00:30.463+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:00:30.464+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:30.464+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:00:30.465+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:30.465+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:00:30.474+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:30.474+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:00:30.475+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:00:30.475+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:30.475+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:00:30.475+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:30.475+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:00:30.484+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:30.484+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:00:30.492+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:30.492+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:00:30.492+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:30.492+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:00:30.502+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:00:30.502+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T14:00:30.508+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.052 seconds
[2024-11-17T14:01:00.856+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:01:00.856+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 1250)
[2024-11-17T14:01:00.857+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:01:00.857+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1250
[2024-11-17T14:01:00.863+0000] {processor.py:186} INFO - Started process (PID=1250) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:01:00.863+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:01:00.864+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:01:00.864+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:01:00.865+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:01:00.865+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:01:00.875+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:01:00.875+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:01:00.875+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:01:00.876+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:01:00.876+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:01:00.876+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:01:00.876+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:01:00.886+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:01:00.886+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:01:00.894+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:01:00.894+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:01:00.894+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:01:00.894+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:01:00.904+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:01:00.904+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17T00:00:00+00:00, run_after=2024-11-18T00:00:00+00:00
[2024-11-17T14:01:00.911+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.056 seconds
[2024-11-17T14:09:12.948+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:12.948+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 179)
[2024-11-17T14:09:12.949+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:12.949+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=179
[2024-11-17T14:09:12.952+0000] {processor.py:186} INFO - Started process (PID=179) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:09:12.952+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:09:12.954+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:12.954+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:09:12.958+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:12.957+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:09:12.966+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:12.966+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:09:12.966+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:09:12.967+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:12.966+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:09:12.967+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:12.967+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:09:12.974+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:12.974+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:09:12.993+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:12.993+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:09:12.993+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:12.993+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:09:13.006+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:13.006+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17 00:00:00+00:00, run_after=2024-11-18 00:00:00+00:00
[2024-11-17T14:09:13.038+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.091 seconds
[2024-11-17T14:09:43.650+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:43.649+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 184)
[2024-11-17T14:09:43.651+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:43.651+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=184
[2024-11-17T14:09:43.661+0000] {processor.py:186} INFO - Started process (PID=184) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:09:43.662+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:09:43.666+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:43.665+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:09:43.709+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:43.709+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:09:43.737+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:43.736+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:09:43.737+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:09:43.738+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:43.738+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:09:43.738+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:43.738+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:09:43.755+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:43.755+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:09:43.772+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:43.772+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:09:43.773+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:43.773+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:09:43.798+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:09:43.797+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17 00:00:00+00:00, run_after=2024-11-18 00:00:00+00:00
[2024-11-17T14:09:43.814+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.165 seconds
[2024-11-17T14:10:14.832+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:14.830+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 361)
[2024-11-17T14:10:14.840+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:14.839+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=361
[2024-11-17T14:10:14.926+0000] {processor.py:186} INFO - Started process (PID=361) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:10:14.927+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:10:14.931+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:14.930+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:10:14.939+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:14.938+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:10:15.019+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:15.017+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:10:15.019+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:10:15.026+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:15.021+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:10:15.030+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:15.030+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:10:15.086+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:15.086+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:10:15.170+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:15.170+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:10:15.171+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:15.171+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:10:15.214+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:15.214+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17 00:00:00+00:00, run_after=2024-11-18 00:00:00+00:00
[2024-11-17T14:10:15.230+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.401 seconds
[2024-11-17T14:10:46.722+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:46.711+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 366)
[2024-11-17T14:10:46.724+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:46.724+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=366
[2024-11-17T14:10:46.875+0000] {processor.py:186} INFO - Started process (PID=366) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:10:46.877+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:10:46.887+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:46.887+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:10:46.920+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:46.919+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:10:47.531+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:47.514+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:10:47.534+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:10:47.545+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:47.545+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:10:47.547+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:47.547+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:10:48.366+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:48.363+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:10:48.769+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:48.768+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:10:48.772+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:48.771+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:10:49.234+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:10:49.233+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17 00:00:00+00:00, run_after=2024-11-18 00:00:00+00:00
[2024-11-17T14:10:49.412+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 2.718 seconds
[2024-11-17T14:11:20.297+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:20.291+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 371)
[2024-11-17T14:11:20.302+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:20.302+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=371
[2024-11-17T14:11:20.367+0000] {processor.py:186} INFO - Started process (PID=371) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:11:20.368+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:11:20.373+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:20.373+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:11:20.380+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:20.380+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:11:20.540+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:20.539+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:11:20.540+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:11:20.541+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:20.541+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:11:20.541+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:20.541+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:11:20.705+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:20.704+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:11:20.842+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:20.842+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:11:20.843+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:20.842+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:11:20.965+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:20.965+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17 00:00:00+00:00, run_after=2024-11-18 00:00:00+00:00
[2024-11-17T14:11:21.039+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.756 seconds
[2024-11-17T14:11:51.554+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:51.549+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 376)
[2024-11-17T14:11:51.557+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:51.557+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=376
[2024-11-17T14:11:51.714+0000] {processor.py:186} INFO - Started process (PID=376) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:11:51.717+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:11:51.732+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:51.731+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:11:51.751+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:51.751+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:11:52.345+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:52.341+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:11:52.346+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:11:52.349+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:52.348+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:11:52.351+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:52.351+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:11:52.735+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:52.735+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:11:52.917+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:52.914+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:11:52.918+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:52.918+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:11:53.062+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:11:53.062+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17 00:00:00+00:00, run_after=2024-11-18 00:00:00+00:00
[2024-11-17T14:11:53.172+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 1.623 seconds
[2024-11-17T14:12:23.754+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:23.747+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 381)
[2024-11-17T14:12:23.756+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:23.756+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=381
[2024-11-17T14:12:23.835+0000] {processor.py:186} INFO - Started process (PID=381) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:12:23.836+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:12:23.838+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:23.838+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:12:23.845+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:23.845+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:12:23.961+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:23.960+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:12:23.961+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:12:23.964+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:23.964+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:12:23.965+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:23.965+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:12:24.089+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:24.089+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:12:24.149+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:24.148+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:12:24.149+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:24.149+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:12:24.209+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:24.209+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17 00:00:00+00:00, run_after=2024-11-18 00:00:00+00:00
[2024-11-17T14:12:24.252+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.507 seconds
[2024-11-17T14:12:54.566+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:54.565+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 386)
[2024-11-17T14:12:54.567+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:54.567+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=386
[2024-11-17T14:12:54.578+0000] {processor.py:186} INFO - Started process (PID=386) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:12:54.579+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:12:54.580+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:54.580+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:12:54.582+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:54.582+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:12:54.610+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:54.610+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:12:54.610+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:12:54.611+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:54.611+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:12:54.611+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:54.611+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:12:54.670+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:54.670+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:12:54.692+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:54.692+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:12:54.693+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:54.693+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:12:54.733+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:12:54.733+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17 00:00:00+00:00, run_after=2024-11-18 00:00:00+00:00
[2024-11-17T14:12:54.780+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.215 seconds
[2024-11-17T14:13:24.944+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:24.943+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 391)
[2024-11-17T14:13:24.945+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:24.945+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=391
[2024-11-17T14:13:24.950+0000] {processor.py:186} INFO - Started process (PID=391) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:13:24.951+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:13:24.951+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:24.951+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:13:24.953+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:24.953+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:13:24.972+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:24.972+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:13:24.973+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:13:24.973+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:24.973+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:13:24.973+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:24.973+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:13:24.992+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:24.991+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:13:25.009+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:25.009+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:13:25.010+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:25.010+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:13:25.032+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:25.032+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17 00:00:00+00:00, run_after=2024-11-18 00:00:00+00:00
[2024-11-17T14:13:25.046+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.103 seconds
[2024-11-17T14:13:55.500+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:55.499+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 396)
[2024-11-17T14:13:55.502+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:55.501+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=396
[2024-11-17T14:13:55.528+0000] {processor.py:186} INFO - Started process (PID=396) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:13:55.528+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:13:55.529+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:55.529+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:13:55.529+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:55.529+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:13:55.535+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:55.534+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:13:55.535+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:13:55.535+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:55.535+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:13:55.535+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:55.535+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:13:55.545+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:55.545+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:13:55.551+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:55.551+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:13:55.551+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:55.551+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:13:55.559+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:13:55.559+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17 00:00:00+00:00, run_after=2024-11-18 00:00:00+00:00
[2024-11-17T14:13:55.566+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.067 seconds
[2024-11-17T14:14:25.898+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:14:25.898+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 401)
[2024-11-17T14:14:25.899+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:14:25.899+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=401
[2024-11-17T14:14:25.904+0000] {processor.py:186} INFO - Started process (PID=401) to work on /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:14:25.904+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/spark_airflow.py for tasks to queue
[2024-11-17T14:14:25.905+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:14:25.905+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:14:25.906+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:14:25.906+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:14:25.915+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:14:25.915+0000] {dagbag.py:554} DEBUG - Loaded DAG <DAG: sparkingFlow>
[2024-11-17T14:14:25.915+0000] {processor.py:925} INFO - DAG(s) 'sparkingFlow' retrieved from /opt/airflow/dags/spark_airflow.py
[2024-11-17T14:14:25.916+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:14:25.916+0000] {dagbag.py:711} DEBUG - Running dagbag.sync_to_db with retries. Try 1 of 3
[2024-11-17T14:14:25.916+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:14:25.916+0000] {dagbag.py:716} DEBUG - Calling the DAG.bulk_sync_to_db method
[2024-11-17T14:14:25.930+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:14:25.930+0000] {serialized_dag.py:168} DEBUG - Checking if DAG (sparkingFlow) changed
[2024-11-17T14:14:25.937+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:14:25.937+0000] {serialized_dag.py:179} DEBUG - Serialized DAG (sparkingFlow) is unchanged. Skipping writing to DB
[2024-11-17T14:14:25.938+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:14:25.938+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-17T14:14:25.948+0000] {logging_mixin.py:190} INFO - [2024-11-17T14:14:25.948+0000] {dag.py:4180} INFO - Setting next_dagrun for sparkingFlow to 2024-11-17 00:00:00+00:00, run_after=2024-11-18 00:00:00+00:00
[2024-11-17T14:14:25.956+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/spark_airflow.py took 0.059 seconds
